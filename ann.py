# -*- coding: utf-8 -*-
"""ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s7yWBdCTqleMYvp4y6BY8pMImSWlF64X
"""

from google.colab import drive
drive.mount('/content/drive')

"""Load data set from google drive"""

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/ml_lab/MLAssignment/final_data_64_64.csv')

"""extract labels and features as lists (first column is taken as labels and remaining all are cosidered as features)"""

y = data.iloc[:, :1].values
X = data.iloc[:, 1:].values

"""converting labesl from list of list to list of elements"""

print(X[0])

import matplotlib.pyplot as plt
image1 = X[100].reshape(64,64)
plt.imshow(image1, cmap='gray')

"""normalizing data"""

max_number = max(max(sublist) for sublist in X)
result_list = [[element / max_number for element in sublist] for sublist in X]

"""dividing data into training and testing"""

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(result_list,y)

import numpy as np
print(len(xtrain),len(ytrain))
print(len(xtrain[0]),ytrain[100])
import matplotlib.pyplot as plt
image1 = np.array(xtrain[100]).reshape(64,64)
plt.imshow(image1, cmap='gray')



"""Neural network"""

import numpy as np

# Layer Class:
# Represents a single layer in a neural network.
class Layer:
  # Constructor:
  # Initializes the Layer object with the provided parameters.
  #
  # Parameters:
  # - nNeurons: The number of neurons in the layer.
  # - weights: The weights matrix representing connections between neurons in the current layer and the previous layer.
  # - bias: The bias vector for the neurons in the current layer.
  # - activation_function: The activation function applied to the output of the layer.
  # - activation_derivative: The derivative of the activation function, used for backpropagation.
  def __init__(self,nNeurons,weights,bias,activation_function,activation_derivative):
    self.nNeurons = nNeurons
    self.weights = weights
    self.bias = bias
    self.activation_function = activation_function
    self.activation_derivative = activation_derivative
    self.delta = None

  # Method: calculate_output
  # Computes the output of the layer given the input data.
  # If input_Layer is True, assumes the layer is the input layer and directly sets the output to the input data.
  # Otherwise, computes the weighted sum of the input data, adds the bias, applies the activation function, and stores the output.
  #
  # Parameters:
  # - data: The input data to the layer.
  # - input_Layer: Boolean flag indicating whether the layer is the input layer.
  #
  # Returns:
  # - output: The output of the layer.
  def calculate_output(self,data,input_Layer=False):
    if(input_Layer):
      self.output = data
    else:
      self.weighted_sum = np.dot(data,self.weights)+self.bias
      self.output = np.array(self.activation_function(self.weighted_sum))
    return self.output



# Network Class:
# Represents a neural network composed of multiple layers.
class Network:
  # Constructor:
  # Initializes the Network object with the number of layers.
  #
  # Parameters:
  # - nLayers: The number of layers in the network.
  def __init__(self,nLayers):
    self.nLayers=nLayers
    self.Layers=[]
    self.pred_output=[]
  # Activation Functions:
  # Define various activation functions used in the network.

  # Linear activation function.
  def linear(self,x):
    return x
  # ReLU (Rectified Linear Unit) activation function.
  def relu(self,x):
    return np.maximum(0,x)
  # Derivative of the ReLU activation function.
  def relu_derivative(self,x):
    return np.where(x < 0, 0, 1)
  # Softmax activation function.
  def softmax(self,x):
    exp_values=np.exp(x)
    expSum=np.sum(exp_values)
    return exp_values/expSum
  # Derivative of the softmax activation function.
  def softmax_derivative(self,x):
    return self.softmax(x) * (1 - self.softmax(x))


  # Method: Create_Network
  # Creates the network architecture by adding layers with specified parameters.
  #
  # Parameters:
  # - nInputs: The number of input neurons to the network.
  def Create_Network(self,nInputs):
    for i in range(self.nLayers-1):
      if i==0:
        weights=np.ones(nInputs)
        bias=np.ones((nInputs))
        self.Layers.append(Layer(nInputs,weights,bias,self.linear, self.linear))
      else:
        nNeurons=int(input(f"enter number of neurons of Layer {i+1} "))
        weights=0.1*np.random.randn(nInputs,nNeurons)
        bias=0.1*np.random.randn((nNeurons))
        self.Layers.append(Layer(nNeurons,weights,bias,self.relu, self.relu_derivative))
        nInputs=nNeurons
    nNeurons=int(input(f"enter number of neurons in output layer "))
    weights=0.1*np.random.randn(nInputs,nNeurons)
    bias=np.zeros((nNeurons))
    self.Layers.append(Layer(nNeurons,weights,bias,self.softmax, self.softmax_derivative))

  # Method: forwardPass
  # Performs a forward pass through the network.
  #
  # Parameters:
  # - Input_data: The input data to be passed through the network.
  #
  # Returns:
  # - Output_data: The output data produced by the network.
  def forwardPass(self,Input_data):
    for i in range(self.nLayers):
      if(i==0):
        Input_data=self.Layers[i].calculate_output(Input_data,True)
      else:
        Input_data=self.Layers[i].calculate_output(Input_data)
    return Input_data

  # Method: calculate_deltas
  # Calculates delta values for each layer in the network during backpropagation.
  #
  # Parameters:
  # - targets: The target values for the current training batch.
  def calculate_deltas(self, targets):
    for i in range(self.nLayers - 1, 0, -1):
      if i == len(self.Layers)-1:
        self.Layers[i].delta = (self.Layers[i].output - targets)
      else:
        self.Layers[i].delta = np.dot(self.Layers[i+1].delta,self.Layers[i+1].weights.T) * self.Layers[i].activation_derivative(self.Layers[i].weighted_sum)

  # Method: Update_Weights
  # Updates the weights and biases of each layer in the network using gradient descent.
  #
  # Parameters:
  # - lr: The learning rate for gradient descent.
  def Update_Weights(self,lr):
    for i in range(self.nLayers - 1, 0, -1):
      self.Layers[i].bias -= np.dot(lr, self.Layers[i].delta)
      self.Layers[i].weights -= self.Layers[i].delta[np.newaxis,:] * (np.dot(lr,self.Layers[i-1].output)[:, np.newaxis]  * self.Layers[i].weights)

  # Method: calculate_error
  # Calculates the error between predicted outputs and target values.
  #
  # Parameters:
  # - targets: The target values for the current training batch.
  # - outputs: The predicted outputs produced by the network.
  #
  # Returns:
  # - loss: The error between predicted outputs and target values.
  def calculate_error(self, targets, outputs):
      epsilon = 1e-15
      outputs = np.clip(outputs, epsilon, 1 - epsilon)
      loss = - np.sum(targets * np.log(outputs))
      return loss

  # Method: backwardPass
  # Performs a backward pass through the network (backpropagation) to update weights and biases.
  #
  # Parameters:
  # - targets: The target values for the current training batch.
  # - lr: The learning rate for gradient descent.
  def backwardPass(self,targets, lr):
    self.calculate_deltas(targets)
    self.Update_Weights(lr)

  # Method: One_hot
  # Encodes the target labels using one-hot encoding.
  #
  # Parameters:
  # - x: The target label to be encoded.
  #
  # Returns:
  # - ans: The one-hot encoded target label.
  def One_hot(self,x):
    ans = []
    for i in range(4):
      if(i==x):
        ans.append(1)
      else:
        ans.append(0)
    return np.array(ans)


  # Method: fit
  # Trains the network on the provided input data and target labels for the specified number of epochs.
  #
  # Parameters:
  # - input_datas: The input data for training.
  # - target_labels: The target labels for training.
  # - epochs: The number of training epochs.
  # - learning_rate: The learning rate for gradient descent.
  def fit(self, input_datas, target_labels, epochs, learning_rate):
      for epoch in range(epochs):
        total_error = 0
        for input_data,target in zip(input_datas, target_labels):
          outputs = self.forwardPass(input_data)
          targets = self.One_hot(target)
          error = self.calculate_error(targets, outputs)
          total_error+=error
          self.backwardPass(targets, learning_rate)
        print(f"Epoch {epoch + 1} / {epochs}, Error: {total_error/len(input_datas)}")

  # Calculate the accuracy of the neural network on the test dataset.
  # Parameters:
  # - xtest (array-like): Input test data.
  # - ytest (array-like): True labels for the test data.
  # Returns:
  # - accuracy (float): Accuracy of the neural network on the test dataset.
  def calculate_accuracy(self, xtest, ytest):
    correct_predictions = 0
    total_samples = len(ytest)
    for input_data, output_data in zip(xtest, ytest):
      output = self.forwardPass(input_data)
      predicted_class = np.argmax(output)
      if predicted_class == output_data and output[predicted_class] >= 0.9:
        correct_predictions += 1
    accuracy = correct_predictions / total_samples
    return accuracy

nLayers=int(input("enter number of layers"))

obj=Network(nLayers)  #object for Network class
obj.Create_Network(4096)  #creates a network by creating layers and appending to Layers list in Network class (this takes input length as argument)

"""Training of NN"""

obj.fit(xtrain, ytrain, 20, 0.005)

acc = 0
for input_data, output_data in zip(result_list, y):
  output = obj.forwardPass(input_data)
  if output[output_data] >= 0.9:
    acc+=1
print(acc/len(y))

import pickle

# Define the path to the .pkl file
pkl_file_path = "/content/drive/MyDrive/ml_lab/MLAssignment/saved_network_64_64.pkl"

# Open the .pkl file for reading in binary mode
with open(pkl_file_path, "rb") as file:
    # Load the contents of the .pkl file
    data = pickle.load(file)

# Now 'data' contains the contents of the .pkl file
# You can process or store the data as needed
print(data)

obj1 = Network(6)
obj1.Create_Network(4096)

"""Trining Model"""

obj.fit(xtrain, ytrain, 20, 0.03)

"""Finding accuracy of NN"""

acc = 0
for input_data, output_data in zip(result_list, y):
  output = obj1.forwardPass(input_data)
  if output[output_data] >= 0.9:
    acc+=1
print(acc/len(y))

"""Observing Outputs of Each Sample"""

# test_data = [image.flatten() for image in xtest]
#for train data
for input_data, output_data in zip(result_list, y):
  print(output_data,"  ",obj.forwardPass(input_data))
print()

"""Testing with single image"""

import cv2
import numpy as np
# Load the image from Google Drive (grayscale)
image_path = '/content/drive/MyDrive/ml_lab/MLAssignment/faces/left/an2i_left_angry_open.jpg'
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# Resize the image to 30x32 pixels
resized_image = cv2.resize(image, (64, 64))

# Flatten the resized image
flattened_image = resized_image.flatten()

# Normalize the pixel values
normalized_image = flattened_image / 255.0

# Store the normalized pixel values in a list
input_image = normalized_image.tolist()

# Display the shape of the normalized pixel values
print("Shape of normalized pixel values:", np.array(input_image).shape)

print(obj.forwardPass(input_image))

"""Saving Model after trining"""

state_dict = {}
for i, layer in enumerate(obj.Layers):
    state_dict[f"layer_{i}_weights"] = layer.weights
    state_dict[f"layer_{i}_bias"] = layer.bias

# Store the state dictionary within the object itself
state = state_dict

print(state['layer_3_weights'].shape)

"""Loading Saved Model"""

import pickle

# Save the serialized state to a file in Google Drive
filename = '/content/drive/MyDrive/ml_lab/MLAssignment/saved_network_64_64_1.pkl'
with open(filename, 'wb') as file:
    pickle.dump(state, file)

filename = '/content/drive/MyDrive/ml_lab/MLAssignment/saved_network_30_32.pkl'
with open(filename, 'rb') as file:
    loaded_state = pickle.load(file)

# Update the state of the network object 'obj' with the loaded state
state2 = loaded_state

print(state2['layer_1_bias'])

"""Loading Trained Model Weights And Biases"""

obj1.Layers[0].weights = data['layer_0_weights']
obj1.Layers[0].bias = data['layer_0_bias']
obj1.Layers[1].weights = data['layer_1_weights']
obj1.Layers[1].bias = data['layer_1_bias']
obj1.Layers[2].weights = data['layer_2_weights']
obj1.Layers[2].bias = data['layer_2_bias']
obj1.Layers[3].weights =data['layer_3_weights']
obj1.Layers[3].bias =data['layer_3_bias']
obj1.Layers[4].weights =data['layer_4_weights']
obj1.Layers[4].bias =data['layer_4_bias']


obj1.Layers[5].weights =data['layer_5_weights']
obj1.Layers[5].bias =data['layer_5_bias']